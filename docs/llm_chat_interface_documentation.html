<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BasementBrewAI - LLM Chat Interface Documentation</title>
    <style>
        :root {
            --orange-bright: #FF8C00;
            --orange-dim: #CC7000;
            --green-bright: #32CD32;
            --green-normal: #28A428;
            --green-dim: #1E7B1E;
            --bg-dark: #0A0A0A;
            --bg-panel: #1A1A1A;
            --bg-code: #252525;
            --text-primary: #E0E0E0;
            --text-secondary: #B0B0B0;
            --border-color: #333;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Courier New', monospace;
            background: linear-gradient(135deg, var(--bg-dark) 0%, #1a0f0f 100%);
            color: var(--text-primary);
            line-height: 1.6;
            padding: 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        header {
            text-align: center;
            padding: 40px 0;
            border-bottom: 3px solid var(--orange-bright);
            margin-bottom: 40px;
            background: var(--bg-panel);
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(255, 140, 0, 0.1);
        }

        h1 {
            color: var(--orange-bright);
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
        }

        .subtitle {
            color: var(--green-dim);
            font-size: 1.2em;
        }

        .ascii-art {
            color: var(--green-bright);
            font-size: 0.8em;
            margin: 20px 0;
            white-space: pre;
            font-family: monospace;
        }

        section {
            background: var(--bg-panel);
            border-radius: 10px;
            padding: 30px;
            margin-bottom: 30px;
            border: 1px solid var(--border-color);
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
        }

        h2 {
            color: var(--orange-bright);
            border-bottom: 2px solid var(--orange-dim);
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }

        h3 {
            color: var(--green-bright);
            margin: 20px 0 15px 0;
            font-size: 1.3em;
        }

        h4 {
            color: var(--green-normal);
            margin: 15px 0 10px 0;
            font-size: 1.1em;
        }

        p {
            margin-bottom: 15px;
            color: var(--text-secondary);
        }

        code {
            background: var(--bg-code);
            color: var(--green-bright);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            border: 1px solid var(--border-color);
        }

        pre {
            background: var(--bg-code);
            border: 1px solid var(--border-color);
            border-radius: 5px;
            padding: 15px;
            overflow-x: auto;
            margin: 15px 0;
            color: var(--green-normal);
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.2);
        }

        .code-block {
            position: relative;
        }

        .code-lang {
            position: absolute;
            top: 5px;
            right: 10px;
            color: var(--orange-dim);
            font-size: 0.9em;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
            color: var(--text-secondary);
        }

        .warning {
            background: rgba(255, 140, 0, 0.1);
            border-left: 4px solid var(--orange-bright);
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .warning strong {
            color: var(--orange-bright);
        }

        .info {
            background: rgba(50, 205, 50, 0.1);
            border-left: 4px solid var(--green-bright);
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .info strong {
            color: var(--green-bright);
        }

        .flow-diagram {
            background: var(--bg-code);
            border: 2px solid var(--green-dim);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th {
            background: var(--bg-code);
            color: var(--orange-bright);
            padding: 12px;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        td {
            padding: 10px;
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tr:nth-child(even) {
            background: rgba(255, 255, 255, 0.02);
        }

        .button-example {
            display: inline-block;
            background: var(--bg-code);
            color: var(--green-bright);
            padding: 8px 16px;
            border: 1px solid var(--green-dim);
            border-radius: 5px;
            margin: 5px;
            font-family: monospace;
        }

        .nav-menu {
            background: var(--bg-code);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 30px;
            position: sticky;
            top: 20px;
            z-index: 100;
        }

        .nav-menu h3 {
            margin-bottom: 15px;
        }

        .nav-menu a {
            color: var(--green-normal);
            text-decoration: none;
            display: block;
            padding: 5px 10px;
            margin: 5px 0;
            border-radius: 3px;
            transition: all 0.3s;
        }

        .nav-menu a:hover {
            background: var(--bg-panel);
            color: var(--green-bright);
            padding-left: 15px;
        }

        footer {
            text-align: center;
            padding: 30px;
            margin-top: 50px;
            border-top: 2px solid var(--orange-dim);
            color: var(--text-secondary);
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .tech-item {
            background: var(--bg-code);
            padding: 15px;
            border-radius: 5px;
            border: 1px solid var(--border-color);
        }

        .tech-item h4 {
            color: var(--orange-bright);
            margin-bottom: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <pre class="ascii-art">
╔══════════════════════════════════════════════════════════════╗
║  ____                                      _   ____                    ║
║ | __ )  __ _ ___  ___ _ __ ___   ___ _ __ | |_| __ ) _ __ _____      __║
║ |  _ \ / _` / __|/ _ \ '_ ` _ \ / _ \ '_ \| __|  _ \| '__/ _ \ \ /\ / /║
║ | |_) | (_| \__ \  __/ | | | | |  __/ | | | |_| |_) | | |  __/\ V  V / ║
║ |____/ \__,_|___/\___|_| |_| |_|\___|_| |_|\__|____/|_|  \___| \_/\_/  ║
╚══════════════════════════════════════════════════════════════╝
            </pre>
            <h1>LLM Chat Interface Documentation</h1>
            <p class="subtitle">Technical Guide & User Manual v1.0</p>
        </header>

        <nav class="nav-menu">
            <h3>[ NAVIGATION ]</h3>
            <a href="#overview">System Overview</a>
            <a href="#architecture">Technical Architecture</a>
            <a href="#user-guide">User Guide</a>
            <a href="#api-reference">API Reference</a>
            <a href="#troubleshooting">Troubleshooting</a>
            <a href="#performance">Performance Tips</a>
        </nav>

        <section id="overview">
            <h2>[ SYSTEM OVERVIEW ]</h2>
            
            <p>The BasementBrewAI LLM Chat Interface is a comprehensive system for loading and interacting with trained language models. It supports both full model checkpoints and LoRA (Low-Rank Adaptation) fine-tuned models, with built-in 4-bit quantization support for memory-efficient inference.</p>

            <div class="info">
                <strong>Key Features:</strong>
                <ul>
                    <li>✓ Automatic model discovery from training outputs</li>
                    <li>✓ Support for LoRA and full model checkpoints</li>
                    <li>✓ 4-bit quantization via bitsandbytes</li>
                    <li>✓ Conversation history management</li>
                    <li>✓ Thread-safe asynchronous generation</li>
                    <li>✓ Safety filtering (with danger mode override)</li>
                </ul>
            </div>

            <h3>Component Overview</h3>
            <div class="flow-diagram">
                <pre>
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│   Dear PyGui    │────▶│  LLMTrainingUI   │────▶│  LLMInference   │
│   Chat Panel    │     │   (llm_ui.py)    │     │ (generation.py) │
└─────────────────┘     └──────────────────┘     └─────────────────┘
         │                       │                         │
         │                       │                         ▼
         ▼                       ▼                  ┌──────────────┐
┌─────────────────┐     ┌──────────────────┐      │ Transformers │
│  User Input     │     │  Model Loader    │      │   Pipeline   │
└─────────────────┘     └──────────────────┘      └──────────────┘
                </pre>
            </div>
        </section>

        <section id="architecture">
            <h2>[ TECHNICAL ARCHITECTURE ]</h2>

            <h3>1. Module Structure</h3>
            <div class="code-block">
                <span class="code-lang">filesystem</span>
                <pre>
trainer/
├── llm_ui.py                    # Main UI controller
├── llm/
│   ├── inference/
│   │   ├── __init__.py          # Module exports
│   │   └── generation.py        # Core inference engine
│   ├── models/
│   │   ├── base.py             # Base LLM class
│   │   └── gpt_oss.py          # GPT-OSS implementations
│   └── training/
│       └── trainer.py          # Training pipeline
└── saved_models/               # Model storage
    └── llm_outputs/           # LoRA checkpoints
                </pre>
            </div>

            <h3>2. Core Classes</h3>

            <h4>LLMInference Class</h4>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
class LLMInference:
    """Manages LLM model loading and text generation."""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
    def load_model(self, checkpoint_path: str = None):
        """Load a trained model from checkpoint."""
        
    def generate(self, prompt: str, **kwargs) -> str:
        """Generate text from a prompt."""
        
    def chat(self, message: str, history: List) -> str:
        """Chat with conversation context."""
                </pre>
            </div>

            <h4>InferenceConfig Dataclass</h4>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
@dataclass
class InferenceConfig:
    model_path: str              # Path to model checkpoint
    device: str = "cuda"         # Computation device
    load_in_4bit: bool = True    # Enable quantization
    max_new_tokens: int = 512    # Generation length
    temperature: float = 0.7     # Sampling temperature
    top_p: float = 0.9          # Nucleus sampling
    apply_safety_filter: bool = True
                </pre>
            </div>

            <h3>3. Model Loading Pipeline</h3>

            <table>
                <thead>
                    <tr>
                        <th>Step</th>
                        <th>Process</th>
                        <th>Components</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1. Discovery</td>
                        <td>Scan directories for models</td>
                        <td><code>llm_outputs/</code>, <code>saved_models/</code></td>
                    </tr>
                    <tr>
                        <td>2. Detection</td>
                        <td>Identify model type</td>
                        <td>Check for <code>adapter_config.json</code> (LoRA)</td>
                    </tr>
                    <tr>
                        <td>3. Configuration</td>
                        <td>Setup quantization</td>
                        <td>BitsAndBytesConfig for 4-bit</td>
                    </tr>
                    <tr>
                        <td>4. Loading</td>
                        <td>Load model & tokenizer</td>
                        <td>AutoModelForCausalLM, AutoTokenizer</td>
                    </tr>
                    <tr>
                        <td>5. LoRA Merge</td>
                        <td>Apply LoRA weights</td>
                        <td>PeftModel.from_pretrained()</td>
                    </tr>
                </tbody>
            </table>

            <h3>4. Threading Model</h3>
            
            <p>The chat interface uses Python's threading to prevent UI blocking during model operations:</p>

            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
# Model loading thread
threading.Thread(target=self._load_model_thread, args=(path,)).start()

# Generation thread
threading.Thread(target=self._generate_response, args=(prompt,)).start()
                </pre>
            </div>

            <div class="warning">
                <strong>Thread Safety:</strong> Dear PyGui UI updates must be performed from threads safely. The implementation uses value setters that are thread-safe.
            </div>

            <h3>5. Memory Management</h3>

            <div class="tech-stack">
                <div class="tech-item">
                    <h4>4-bit Quantization</h4>
                    <p>Reduces model size by ~75% using NF4 quantization</p>
                </div>
                <div class="tech-item">
                    <h4>Device Mapping</h4>
                    <p>Automatic GPU/CPU distribution with device_map="auto"</p>
                </div>
                <div class="tech-item">
                    <h4>Gradient Checkpointing</h4>
                    <p>Disabled during inference for speed</p>
                </div>
                <div class="tech-item">
                    <h4>Cache Management</h4>
                    <p>torch.cuda.empty_cache() on model unload</p>
                </div>
            </div>
        </section>

        <section id="user-guide">
            <h2>[ USER GUIDE ]</h2>

            <h3>Getting Started</h3>

            <ol>
                <li><strong>Launch the Application</strong>
                    <pre>python run.py</pre>
                </li>
                <li><strong>Navigate to LLM Lab Tab</strong>
                    <p>Click on the "LLM Lab" tab in the main interface</p>
                </li>
                <li><strong>Locate the Chat Panel</strong>
                    <p>The rightmost panel contains the chat interface</p>
                </li>
            </ol>

            <h3>Loading a Model</h3>

            <h4>Step 1: Click Load Model</h4>
            <p>Click the <span class="button-example">[↓] Load Model</span> button in the chat interface.</p>

            <h4>Step 2: Select Your Model</h4>
            <p>The model loader will display:</p>
            <ul>
                <li>Trained LoRA models from <code>llm_outputs/</code></li>
                <li>Full models from <code>saved_models/</code></li>
                <li>Option to browse for custom paths</li>
            </ul>

            <h4>Step 3: Configure Loading Options</h4>
            <table>
                <tr>
                    <th>Option</th>
                    <th>Description</th>
                    <th>Recommendation</th>
                </tr>
                <tr>
                    <td>Load in 4-bit</td>
                    <td>Reduces VRAM usage by ~75%</td>
                    <td>✓ Enable for models >7B params</td>
                </tr>
            </table>

            <h4>Step 4: Wait for Loading</h4>
            <p>Model loading typically takes 10-60 seconds depending on size and quantization.</p>

            <div class="info">
                <strong>Loading Indicators:</strong>
                <ul>
                    <li>Status text: "Loading model... This may take a moment."</li>
                    <li>Model status updates to show loaded model name</li>
                    <li>Chat output clears and shows success message</li>
                </ul>
            </div>

            <h3>Chatting with the Model</h3>

            <h4>Basic Chat</h4>
            <ol>
                <li>Type your message in the input field</li>
                <li>Press Enter or click <span class="button-example">Send</span></li>
                <li>Wait for generation (shows "[Generating...]")</li>
                <li>Response appears prefixed with "&lt;&lt; Model:"</li>
            </ol>

            <h4>Conversation Context</h4>
            <p>The system maintains conversation history automatically:</p>
            <ul>
                <li>Last 20 messages are kept in context</li>
                <li>Both user and model messages are tracked</li>
                <li>Context resets when loading a new model</li>
            </ul>

            <h3>Advanced Features</h3>

            <h4>Danger Mode Integration</h4>
            <p>When danger mode is activated in the safety controls:</p>
            <ul>
                <li>Safety filtering is disabled for generation</li>
                <li>Model outputs are unfiltered</li>
                <li>Use with extreme caution</li>
            </ul>

            <h4>Custom Model Paths</h4>
            <p>To load models from custom locations:</p>
            <ol>
                <li>Click Browse in the model loader</li>
                <li>Navigate to your model directory</li>
                <li>Ensure directory contains:
                    <ul>
                        <li>For LoRA: <code>adapter_config.json</code></li>
                        <li>For full models: <code>config.json</code> and weights</li>
                    </ul>
                </li>
            </ol>
        </section>

        <section id="api-reference">
            <h2>[ API REFERENCE ]</h2>

            <h3>LLMInference Methods</h3>

            <h4>load_model(checkpoint_path: str = None)</h4>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
# Load a model from specified path
inference.load_model("/path/to/model")

# Auto-detect LoRA vs full model
# Handles quantization based on config
# Raises FileNotFoundError if path invalid
                </pre>
            </div>

            <h4>generate(prompt: str, **kwargs) -> str</h4>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
# Generate text with custom parameters
response = inference.generate(
    prompt="Hello, how are you?",
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.95
)
                </pre>
            </div>

            <h4>chat(message: str, conversation_history: List) -> str</h4>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
# Chat with conversation context
history = [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"}
]
response = inference.chat("How's the weather?", history)
                </pre>
            </div>

            <h3>Configuration Parameters</h3>

            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Type</th>
                        <th>Default</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>max_new_tokens</td>
                        <td>int</td>
                        <td>512</td>
                        <td>Maximum tokens to generate</td>
                    </tr>
                    <tr>
                        <td>temperature</td>
                        <td>float</td>
                        <td>0.7</td>
                        <td>Sampling randomness (0.0-2.0)</td>
                    </tr>
                    <tr>
                        <td>top_p</td>
                        <td>float</td>
                        <td>0.9</td>
                        <td>Nucleus sampling threshold</td>
                    </tr>
                    <tr>
                        <td>top_k</td>
                        <td>int</td>
                        <td>50</td>
                        <td>Top-k sampling limit</td>
                    </tr>
                    <tr>
                        <td>repetition_penalty</td>
                        <td>float</td>
                        <td>1.1</td>
                        <td>Penalty for repeated tokens</td>
                    </tr>
                    <tr>
                        <td>do_sample</td>
                        <td>bool</td>
                        <td>True</td>
                        <td>Enable sampling vs greedy</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="troubleshooting">
            <h2>[ TROUBLESHOOTING ]</h2>

            <h3>Common Issues</h3>

            <h4>1. CUDA Out of Memory</h4>
            <div class="warning">
                <strong>Error:</strong> "CUDA out of memory. Tried to allocate..."
            </div>
            <p><strong>Solutions:</strong></p>
            <ul>
                <li>Enable 4-bit quantization when loading</li>
                <li>Close other GPU applications</li>
                <li>Use a smaller model variant</li>
                <li>Reduce max_new_tokens</li>
            </ul>

            <h4>2. Model Not Found</h4>
            <div class="warning">
                <strong>Error:</strong> "Model path not found"
            </div>
            <p><strong>Solutions:</strong></p>
            <ul>
                <li>Verify model exists in specified directory</li>
                <li>Check for required files (adapter_config.json for LoRA)</li>
                <li>Use absolute paths when browsing</li>
            </ul>

            <h4>3. Slow Generation</h4>
            <p><strong>Causes & Solutions:</strong></p>
            <table>
                <tr>
                    <th>Cause</th>
                    <th>Solution</th>
                </tr>
                <tr>
                    <td>CPU inference</td>
                    <td>Ensure CUDA is available and model loads on GPU</td>
                </tr>
                <tr>
                    <td>Large model</td>
                    <td>Use 4-bit quantization or smaller variant</td>
                </tr>
                <tr>
                    <td>Long context</td>
                    <td>Reduce conversation history or max_new_tokens</td>
                </tr>
            </table>

            <h4>4. Import Errors</h4>
            <div class="warning">
                <strong>Error:</strong> "No module named 'transformers'"
            </div>
            <p><strong>Solution:</strong> Install required dependencies:</p>
            <pre>pip install -r requirements_llm.txt</pre>

            <h3>Debug Mode</h3>
            <p>Enable detailed logging for troubleshooting:</p>
            <div class="code-block">
                <span class="code-lang">python</span>
                <pre>
import logging
logging.basicConfig(level=logging.DEBUG)

# Check model info after loading
info = inference.get_model_info()
print(info)
                </pre>
            </div>
        </section>

        <section id="performance">
            <h2>[ PERFORMANCE OPTIMIZATION ]</h2>

            <h3>VRAM Usage by Model Size</h3>
            <table>
                <thead>
                    <tr>
                        <th>Model Size</th>
                        <th>Full Precision</th>
                        <th>4-bit Quantized</th>
                        <th>Recommended GPU</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>7B params</td>
                        <td>~14GB</td>
                        <td>~4GB</td>
                        <td>RTX 3060 (12GB)</td>
                    </tr>
                    <tr>
                        <td>13B params</td>
                        <td>~26GB</td>
                        <td>~7GB</td>
                        <td>RTX 3080 (10GB)</td>
                    </tr>
                    <tr>
                        <td>20B params</td>
                        <td>~40GB</td>
                        <td>~10GB</td>
                        <td>RTX 4070 Ti (12GB)</td>
                    </tr>
                    <tr>
                        <td>70B params</td>
                        <td>~140GB</td>
                        <td>~35GB</td>
                        <td>A100 (40GB)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Optimization Tips</h3>

            <h4>For Speed:</h4>
            <ul>
                <li>Disable do_sample for deterministic generation</li>
                <li>Reduce max_new_tokens to minimum needed</li>
                <li>Use flash attention if available</li>
                <li>Keep conversation history under 10 messages</li>
            </ul>

            <h4>For Quality:</h4>
            <ul>
                <li>Increase temperature for creativity (0.8-1.0)</li>
                <li>Use top_p=0.95 for better diversity</li>
                <li>Adjust repetition_penalty (1.1-1.3)</li>
                <li>Provide clear, detailed prompts</li>
            </ul>

            <h4>For Memory:</h4>
            <ul>
                <li>Always use 4-bit for models >7B</li>
                <li>Clear GPU cache between model loads</li>
                <li>Use CPU offloading for very large models</li>
                <li>Limit batch size to 1 for inference</li>
            </ul>

            <h3>Benchmarks</h3>
            <p>Typical generation speeds on RTX 4060 (8GB):</p>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Quantization</th>
                    <th>Tokens/sec</th>
                </tr>
                <tr>
                    <td>GPT-2</td>
                    <td>None</td>
                    <td>~120</td>
                </tr>
                <tr>
                    <td>LLaMA-7B</td>
                    <td>4-bit</td>
                    <td>~35</td>
                </tr>
                <tr>
                    <td>GPT-OSS-20B</td>
                    <td>4-bit</td>
                    <td>~15</td>
                </tr>
            </table>
        </section>

        <footer>
            <p>BasementBrewAI LLM Chat Interface Documentation v1.0</p>
            <p class="ascii-art">
═══════════════════════════════════════════════════════════════
        "Brewing intelligence, one token at a time..."
═══════════════════════════════════════════════════════════════
            </p>
            <p>© 2024 BasementBrewAI - Industrial ML Terminal</p>
        </footer>
    </div>
</body>
</html>